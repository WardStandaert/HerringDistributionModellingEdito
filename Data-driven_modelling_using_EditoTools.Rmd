---
title: "Predictive modelling of Atlantic herring larvae in the Northeast Atlantic Ocean"
author: "Ward Standaert"
date: "2024-05-22"
output:
  html_document:
    css: bootstrap.css
    self_contained: false
    toc: true
    toc_float:
      collapsed: false
    toc_depth: 2
    number_sections: false
    theme: default
    highlight: default
    df_print: paged
    fig_width: 10
    fig_height: 6
bibliography: references.bib
csl: apa.csl
---
  
  ```{=html}
<style>
  .html-widget {
    margin: auto;
  }
</style>
  ```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/home/onyxia/work/HerringDistributionModellingEdito/")

# install and load packages
pckgs <- c("arrow", "tidyverse", "doParallel", "rasterVis", "mapview", 
           "ENMeval", "dynamicSDM", "gridExtra", "maxnet", "raster", "spThin")

installed_packages <- pckgs %in% rownames(installed.packages())

if (any(installed_packages == FALSE)) {
  install.packages(pckgs[!installed_packages])
}

invisible(lapply(pckgs, library, character.only = TRUE))
rm(pckgs)

#Rarr package needs separate install
if (!require("BiocManager", quietly = TRUE))
  install.packages("BiocManager")

BiocManager::install("Rarr")
library("Rarr")


```

# Introduction

Where do you think autumn-spawning Atlantic herring (*Clupea harengus*) spawns? In order to approximate the spawning distribution of Atlantic herring, species distribution models were created for larvae of herring.

# 1. Collect data and build model

## 1.1 Collect & organize occurrence data

We collect data of Atlantic herring larvae occurrences from 2000 - 2020 from the International Herring Larvae Surveys ([IHLS](https://www.ices.dk/data/data-portals/Pages/Eggs-and-larvae.aspx)). The occurrence dataset is read from a parquet file stored in the data lake.

```{r step 1,  fig.width = 7}
# the file to process
acf <- S3FileSystem$create(
  anonymous = T,
  scheme = "https",
  endpoint_override = "s3.waw3-1.cloudferro.com"
)

eurobis <- arrow::open_dataset(acf$path("emodnet/biology/eurobis_occurence_data/eurobisgeoparquet/eurobis_no_partition_sorted.parquet" ))

# Atlantic herring has the aphiaID 126417
# the specific dataset international Herring Larvae Surveys has the id 4423
df_occs <- eurobis |> 
  filter(aphiaidaccepted==126417, datasetid==4423,
         longitude > -12, longitude < 10,
         latitude > 48, latitude < 62,
         observationdate >= as.POSIXct("2000-01-01"),
         observationdate <= as.POSIXct("2020-12-31")) |> 
  collect() 

# select relevant variables from occurrence dataset
df_occs <- df_occs %>% 
  dplyr::select(Latitude=latitude,
                Longitude=longitude, 
                Time=observationdate) %>%
  mutate(year = year(Time),
         month = month(Time))

# plot
mapview(df_occs %>% dplyr::select(Longitude) %>% pull,
        df_occs %>% dplyr::select(Latitude) %>% pull,
        crs = "epsg:4326")
```

<br/> We start with `r nrow(df_occs)` occurrences from the IHLS. These occurrences are present in the following months:
  
```{r step 1.2,  fig.width = 7}
table(df_occs$month)
```

## 1.2 Remove duplicates

```{r step 2}
df_occs <- df_occs %>%
  distinct(year, month, Longitude, Latitude, .keep_all = TRUE)
```

After removal of duplicates, `r nrow(df_occs)` occurrences are left.

## 1.3 Reduce sampling bias

To account for sampling bias in the occurrence dataset, a spatial filtering technique is applied [@vollering2019bunching]. Here we thinned the occurrences so that each pair of occurrences has a minimum distance of 10 nautical miles or 18.52 km. This distance corresponds to the recommended distance between valid hauls in the DATRAS trawl surveys [@international2015manual].

```{r step 3}
cl <- makeCluster(detectCores())
registerDoParallel(cl)

set.seed(123)

df_occs_thinned <- df_occs[0,] %>% dplyr::select(-Time)

# loop through all timesteps and thin the dataset at each step using parallel processing
df_occs_thinned <- 
  foreach(y = 1:length(2000:2020), .combine = rbind) %:%
  foreach(m = 1:12, .combine = rbind, .packages = "dplyr") %dopar% {
    tmp_df <- df_occs |> filter(year == c(2000:2020)[y], month == m)
    
    tmp_df_thinned <- spThin::thin(mutate(tmp_df, species = "Atlantic herring"),
                                   lat.col = "Latitude", long.col = "Longitude",
                                   spec.col = "species", thin.par = 18.52, reps = 1,
                                   write.files = FALSE, locs.thinned.list.return = TRUE,
                                   verbose = FALSE, write.log.file = FALSE)[[1]]
    
    tmp_df_thinned |> mutate(
      year = c(2000:2020)[y],
      month = m)
    
  }

```

The thinning of the occurrences results in a remaining number of `r nrow(df_occs_thinned)` observations.

```{r step 3.2, fig.width = 7}
mapview(df_occs_thinned %>% dplyr::select(Longitude) %>% pull,
        df_occs_thinned %>% dplyr::select(Latitude) %>% pull,
        crs = "epsg:4326")

```

<br/> After dealing with spatial bias, `r nrow(df_occs_thinned)` occurrences remain for modelling.

## 1.4 Create background points

We will use Maximum entropy (Maxent) models to fit the habitat suitability of herring larvae [@phillips2004maximum]. This approach makes use of background points instead of (pseudo-)absences to characterize the study area [@phillips2009sample]. The following chunk derives 10000 background points from within a buffer of 100 km around the occurrences [@barve2011crucial].

```{r step 4}
set.seed(123)
#this takes about half a minute
abs <- spatiotemp_pseudoabs(spatial.method = "buffer", temporal.method = "random",
                            occ.data = df_occs_thinned %>% mutate(x = Longitude, y = Latitude),
                            temporal.ext = c("2000-01-01", "2020-12-31"), spatial.buffer = 100000,
                            n.pseudoabs = 10000)
glimpse(abs)
```

For this example, we do not extrapolate the model to months where we have no information on the larvae. For this reason, the background points are sampled from the months where occurrences of larvae are present.

```{r step 4.2}
abs <- abs %>%
  mutate(month = sample(unique(df_occs_thinned$month), nrow(abs), replace = TRUE)) %>%
  mutate(Longitude = x, Latitude = y) %>%
  dplyr::select(-day, -x, -y)
glimpse(abs)

```

## 1.5 Extract environmental values at occurrences and background points

At each occurrence and background point, we want to know what the value is of the environmental variables at this location and time.

Traditionally, this is proceeded by a cumbersome process of looking for and downloading relevant maps for each parameter. Consequently, environmental values could be extracted at point locations from these maps. Since lots of environmental data are now stored in the datalake of EDITO as .zarr files. These .zarr files can be accessed efficiently, directly at the location and time needed.

The spatial resolution of environmental variables should match the resolution of the species records during species distribution modelling [@sillero2021common]. To solve this, environmental maps are traditionally resampled to the same resolution as the species records. The extraction method from .zarr files on EDITO allows the user to provide a buffer. When a buffer is provided, environmental values are not only extracted at the requested longitude, latitude and time coordinate but from all cells within the specified buffer around the location. Next, a user specified function (i.e. mean, max, min, median) is applied to these values to summarize this information into one value for each requested longitude, latitude and time coordinate. This way, an uncertainty is added to the location of the occurrence / (pseudo-)absence / background point. In this example, we apply a buffer of 10 NM, which is the resolution of the occurrence data.

```{r step 5, results='hide'}
#combine occurrences and background points
df_occ_bg <- rbind(df_occs_thinned %>% mutate(presence = 1), 
                   abs %>% mutate(presence = 0))

source("editoTools.R", echo = FALSE)
options("outputdebug"=NULL)

parameters = list("thetao"= c("par" = "thetao", "fun" = "mean", "buffer" = "18000"),
                  "so"= c("par" = "so", "fun" = "mean", "buffer" = "18000"),
                  "zooc"= c("par" = "zooc", "fun" = "mean", "buffer" = "18000", "convert_from_timestep" = 86400000),
                  "phyc"= c("par" = "phyc", "fun" = "mean", "buffer" = "18000"))

#check if they are all available in the data lake
for (parameter in parameters) {
  param = ifelse(length(parameter) == 1, parameter, parameter["par"])
  if(! param %in% unique(EDITOSTAC$par)) dbl("Unknown parameter ", param)
}

#extract function (requires POSIXct Time column)
#this takes about 8 minutes for 14000 points
df_occ_bg_env = enhanceDF(inputPoints = df_occ_bg %>% 
                            mutate(Time = as.POSIXct(paste(year,month,1,sep = "-"))),
                          requestedParameters = parameters,
                          requestedTimeSteps = NA,
                          stacCatalogue = EDITOSTAC,
                          verbose="on",
                          select_layers = rep(1,4))

df_occ_bg_env <- df_occ_bg_env %>% dplyr::select(presence, Longitude, Latitude, year, month,
                                                 thetao, so, zooc, phyc)
```

This part shows how to add your own product and sample data from this product. 
Here I use a downsampled product of elevation, stored in the datalake, to increase computing time.

```{r step 5.2, results='hide'}
#add the new bathymetry file to the catalog (dataframe called EDITOSTAC that is loaded while sourcing EditoTools.R)
url <- "https://minio.lab.dive.edito.eu/oidc-willemboone/bathymetry_ward.zarr"
EDITOSTAC_ed <- EDITOSTAC %>% add_row(par = "elevation",
                      href = url,
                      latmin = 40,
                      latmax = 62,
                      lonmin = -12,
                      lonmax = 10,
                      start_datetime = "1900-01-01",
                      end_datetime = "3000-01-01",
                      title = "downsampled_bathymetry",
                      categories = 0,
                      catalogue = "EMODNET",
                      chunktype = "chunked")

parameters = list("elevation"= c("par" = "elevation",
                                 "fun" = "mean",
                                 "buffer" = "18000"))

df_occ_bg_env2 = enhanceDF(inputPoints = df_occ_bg_env %>%
                                mutate(Time = as.POSIXct(paste(year,month,1,sep = "-"))),
                              requestedParameters = parameters, 
                              requestedTimeSteps = NA, 
                              stacCatalogue = EDITOSTAC_ed, 
                              verbose="on",
                              atDepth = 20,
                              select_layers = 2)

df_occ_bg_env2 <- df_occ_bg_env2 %>% dplyr::select(presence, Longitude, Latitude, year, month,
                                                   thetao, so, zooc, phyc, elevation)
# remove observations where no environmental values were present
df_occ_bg_env <- drop_na(df_occ_bg_env2)
```

``` {r step 5.3}
table(df_occ_bg_env$presence)
```

## 1.6 Model creation using ENMeval

Maxent can be tailored by combining different feature classes and regularization multipliers [@phillips2006modelling]. Fifteen combinations were tested using the corrected Akaike's Information Criterion (AICc) as a selection criterion [@zeng2016novel].

```{r step 6}
# input for ENMevaluate requires lon & lat as first covariates

#test different model settings using ENMeval
#this takes about 20 mins
model_fit <- ENMeval::ENMevaluate(occs = df_occ_bg_env %>%                                    
                                    filter(presence == 1) %>% 
                                    dplyr::select(Longitude, Latitude, thetao, so, zooc, phyc, elevation),
                                  bg = df_occ_bg_env %>% 
                                    filter(presence == 0) %>% 
                                    dplyr::select(Longitude, Latitude, thetao, so, zooc, phyc, elevation),
                                  tune.args = list(fc = c("L","LQ","LQH"),
                                                   rm = c(1,2,4,8,32)),
                                  algorithm = "maxnet",
                                  partitions = "randomkfold",
                                  doClamp = TRUE,
                                  parallel = TRUE)
```

The default output of the *ENMeval* package shows the Area Under the Curve of the Receiver Operating Characteristic plot (AUC) metric as an evaluation metric. This metric was derived using a 5-fold cross validation.

```{r step 7.2}
print(model_fit %>% eval.results() %>% filter(delta.AICc == 0))
```

The outcomes show that the lowest AICc score is achieved using a regularization multiplier of 1 and the feature combinations of linear, quadratic and hinge. Using 5-fold cross-validation, the model has an AUC score of `r model_fit %>% eval.results() %>% filter(delta.AICc == 0) %>% dplyr::select(auc.val.avg) %>% round(2) %>% pull()`, which implies a good fit according to @krzanowski2009roc.

## 1.7 Response curves

Visualize the partial response curves of the occurrence of larvae of herring per environmental variable, using a bootstrapping method derived from @thuiller2009biomod.

```{r step 10, warning=FALSE}
#derive optimal model results
eval_res <- model_fit
opt.aicc <- eval.results(model_fit) %>% filter(delta.AICc == 0)
mod <- eval_res@models[[which(names(eval_res@models) == opt.aicc$tune.args)]]

#variables as defined in the model
vs <- c("thetao", "so", "phyc", "zooc", "elevation")

#variables names for the plot
name_key <- data.frame(old = c("thetao", "so", "phyc", "zooc", "elevation"),
                       new = c("Sea surface temperature (°C)%", "Sea surface salinity (PSU)%", 
                               "Phytoplankton concentration%(mmol C / m³)", "Zooplankton concentration%(g C / m²)",
                               "Depth (m)"))

#function to add a line break at %'s inputs
addline_format <- function(x,...){
  gsub('%','\n',x)
}

#loops through all variables and creates a plot
for (i in 1:nrow(name_key)) {
  v <- name_key$old[i]
  out_name <- name_key$new[i]
  
  dat <- response.plot(mod, v, type = "cloglog", 
                       ylab = "Probability of occurrence",
                       plot = F)
  
  if(is.character(dat[1,1])) {
    assign(v, ggplot(dat) +
             geom_bar(aes_string(x = v, y = "pred"), stat='identity', fill = "#332288") +
             scale_y_continuous(limits = c(0,1)) +
             coord_flip() +
             labs(title = out_name, x = "", y = "Probability of presence") +
             theme_bw() +
             theme(legend.title = element_blank(),
                   plot.title = element_text(size=10, face = "bold", colour = "black", hjust = 0.5),
                   axis.text.y = element_text(size=10, face = "plain", colour = "black"),
                   axis.text.x = element_text(size=10, face = "plain", colour = "black"),
                   axis.title.x = element_text(size=10, face = "bold", colour = "black"),
                   axis.title.y = element_text(size=10, face = "bold", colour = "black"),
                   legend.text = element_text(size=10, face = "bold", colour = "black")))
  } else {
    
    #define plot bounds (restricted to where occurrences are present)
    min <- df_occ_bg_env %>% filter(presence == 1) %>% dplyr::select(all_of(v)) %>% min
    max <- df_occ_bg_env %>% filter(presence == 1) %>% dplyr::select(all_of(v)) %>% max
    
    assign(v, ggplot(dat) +
             geom_line(aes_string(x = v, y = "pred"), linewidth = 1, color = "#332288") + 
             labs(x = addline_format(out_name), y = "Probability of presence") +
             scale_x_continuous(limits = c(min, max)) +
             scale_y_continuous(limits = c(0,1)) +
             theme_bw() +
             theme(legend.title = element_text(size=10, face = "bold", colour = "black"),
                   legend.text = element_text(size=10, face = "plain", colour = "black"), 
                   axis.text.y = element_text(size=10, face = "plain", colour = "black"),
                   axis.text.x = element_text(size=10, face = "plain", colour = "black"),
                   axis.title.x = element_text(size=10, face = "bold", colour = "black"),
                   axis.title.y = element_text(size=10, face = "bold", colour = "black")))
  }
}

grid.arrange(grobs = list(thetao, so + ylab(""), 
                          phyc, zooc + ylab(""),
                          elevation))
```

# 3. Project model

## 3.1 Get raster slices to project on

Finally, we want to make spatial predictions for the habitat suitability in the Northeast Atlantic. This is done by combining the created model with maps of the environmental variables. We will make a projection of the model for January 2020 here.

First, maps are derived from the .zarr files in the datalake at a given time and between a set of longitude and latitudes (defined by the study area).

```{r step 8, results='hide'}
variables <- c("thetao", "so", "phyc", "zooc", "elevation")

#get model
eval_res <- model_fit
opt.aicc <- eval.results(model_fit) %>% filter(delta.AICc == 0)
mod <- eval_res@models[[which(names(eval_res@models) == opt.aicc$tune.args)]]

raster_list <- list()
st <- list()
for (d in 1:4) {
  for (i in 1:length(variables)) {
  raster_list[[i]] <- getRasterSlice(variables[i],
                                      stacCatalogue = EDITOSTAC_ed,
                                      lon_min = -12,
                                      lon_max = 10,
                                      lat_min = 48,
                                      lat_max = 62,
                                      requestedTimeSteps = NA,
                                      date = c("2019-09-01", "2019-10-01", "2019-12-01", "2020-01-01")[d],
                                      select_layers = c(1,1,1,1,2))
  }
  resolutions <- sapply(raster_list, res, simplify = TRUE)
  coarsest_resolution <- resolutions[, which.max(apply(resolutions, 2,
                                                       function(x) max(x,
                                                                       na.rm= TRUE)))]
  #This is the common most coarse resolution
  coarsest_resolution
  r_coarsest_resolution <- raster_list[[which.max(apply(resolutions, 2, function(x) max(x, na.rm = TRUE)))]]
  
  l <- lapply(raster_list, FUN = function(x) raster(resample(x, r_coarsest_resolution)))
  st[[d]] <- stack(l)
  names(st[[d]]) <- variables
  
}

```


```{r step 8.2}
plot(st[[1]])
```

## 3.2 Spatial predictions

Spatial predictions are restricted to the months were occurrence data of herring larvae is available (September, October, December and January). This is the same period where autumn-spawning herring spawn.

```{r step 9}
predictions <- stack()
for (d in 1:4) {
  pred_m <- predict(st[[d]], mod, clamp=T, type="cloglog")
  # crop to extent of study area
  pred_m <- crop(pred_m, extent(-12, 10, 48, 62))
  
  names(pred_m) <- paste0("prediction_", c("2019-09-01", "2019-10-01", "2019-12-01", "2020-01-01")[d])
  
  predictions <- stack(predictions, pred_m)
}

gplot(predictions) +
  geom_tile(aes(fill = value)) +
  facet_wrap(~ variable) +
  labs(x = "", y = "") +
  scale_fill_gradientn(colours = hcl.colors(225, "Viridis")) +
  coord_equal() +
  theme_minimal()

```

# 4. References
